#!/usr/bin/env python3
"""
Enhanced Step 4: Outbreak Monitoring Pipeline
=============================================

This script implements the final step of our data-driven outbreak monitoring system.
It combines all previous steps to create a comprehensive pipeline that:

1. Extracts location and time information from news headlines
2. Geocodes locations for spatial analysis  
3. Builds time series of outbreak events
4. Trains a classifier to distinguish real outbreaks from noise
5. Visualizes results with interactive dashboards

Key Components:
- NLP Processing: spaCy for entity recognition
- Geospatial Analysis: GeoPy for location mapping
- Time Series Analysis: Pandas for temporal patterns
- Machine Learning: Scikit-learn for classification
- Visualization: Matplotlib/Seaborn for insights

Learning Objectives:
- Understand end-to-end data science workflows
- Practice real-world epidemiological data analysis
- Learn rapid-response techniques for public health emergencies
"""

# =============================================================================
# 1. ENVIRONMENT SETUP AND IMPORTS
# =============================================================================

print("ğŸš€ Starting Enhanced Outbreak Monitoring Pipeline...")

# Standard data science libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# NLP and text processing
import spacy
import re
import unidecode

# Geospatial analysis
import geopy
from geopy.geocoders import Nominatim
import geonamescache

# Machine learning
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Visualization
import matplotlib.dates as mdates
from datetime import datetime, timedelta

# Set up plotting style for better visualizations
plt.style.use('default')
sns.set_palette("husl")

print("âœ… All libraries imported successfully!")
print(f"ğŸ“Š Pandas version: {pd.__version__}")
print(f"ğŸ§  spaCy version: {spacy.__version__}")
print(f"ğŸŒ GeoPy version: {geopy.__version__}")

# =============================================================================
# 2. DATA LOADING AND INITIAL EXPLORATION
# =============================================================================

print("\nğŸ“‹ Loading and exploring data...")

# Sample outbreak headlines (synthetic data for demonstration)
# In practice, you would load your actual processed data from previous steps
sample_headlines = [
    "Zika Outbreak Hits Miami Beach",
    "Dengue Cases Rise in Singapore", 
    "Malaria Outbreak in Rural Kenya",
    "COVID-19 Cases Spike in New York",
    "Flu Season Peaks in London",
    "Ebola Cases Reported in Congo",
    "Cholera Outbreak in Haiti",
    "Yellow Fever Cases in Brazil",
    "Measles Outbreak in California",
    "Typhoid Cases in India"
]

# Create a sample DataFrame
df = pd.DataFrame({
    'headline': sample_headlines,
    'date': pd.date_range('2024-01-01', periods=len(sample_headlines), freq='D'),
    'location': ['Miami', 'Singapore', 'Kenya', 'New York', 'London', 'Congo', 'Haiti', 'Brazil', 'California', 'India'],
    'disease': ['Zika', 'Dengue', 'Malaria', 'COVID-19', 'Flu', 'Ebola', 'Cholera', 'Yellow Fever', 'Measles', 'Typhoid'],
    'severity': np.random.choice(['Low', 'Medium', 'High'], size=len(sample_headlines)),
    'cases': np.random.randint(10, 1000, size=len(sample_headlines))
})

print("ğŸ“‹ Sample Dataset Structure:")
print(f"Shape: {df.shape}")
print("\nğŸ” First few rows:")
print(df.head())

print("\nğŸ“Š Dataset Info:")
print(df.info())

print("\nğŸ“ˆ Summary Statistics:")
print(df.describe())

# =============================================================================
# 3. ADVANCED LOCATION PROCESSING
# =============================================================================

print("\nğŸŒ Processing locations...")

# Initialize geocoding services
geolocator = Nominatim(user_agent="outbreak_monitor")
gc = geonamescache.GeonamesCache()

def enhanced_location_processing(location_text):
    """
    Enhanced location processing with multiple fallback strategies.
    
    Args:
        location_text (str): Raw location text from headline
    
    Returns:
        dict: Processed location information
    """
    
    # Clean the location text
    clean_location = unidecode.unidecode(location_text.strip())
    
    # Try to geocode the location
    try:
        location = geolocator.geocode(clean_location, timeout=10)
        if location:
            return {
                'original': location_text,
                'cleaned': clean_location,
                'latitude': location.latitude,
                'longitude': location.longitude,
                'address': location.address,
                'confidence': 'high'
            }
    except Exception as e:
        print(f"âš ï¸ Geocoding failed for {clean_location}: {e}")
    
    # Fallback: return basic info
    return {
        'original': location_text,
        'cleaned': clean_location,
        'latitude': None,
        'longitude': None,
        'address': clean_location,
        'confidence': 'low'
    }

# Process locations for our dataset
print("ğŸŒ Processing locations...")
location_data = []

for idx, row in df.iterrows():
    location_info = enhanced_location_processing(row['location'])
    location_data.append(location_info)
    
    # Progress indicator
    if (idx + 1) % 5 == 0:
        print(f"   Processed {idx + 1}/{len(df)} locations")

# Add location data to DataFrame
location_df = pd.DataFrame(location_data)
df = pd.concat([df, location_df], axis=1)

print("\nâœ… Location processing complete!")
print(f"ğŸ“ Successfully geocoded: {df['confidence'].value_counts().get('high', 0)} locations")
print(f"âš ï¸ Low confidence: {df['confidence'].value_counts().get('low', 0)} locations")

# =============================================================================
# 4. TIME SERIES ANALYSIS
# =============================================================================

print("\nğŸ“… Analyzing temporal patterns...")

# Time series analysis
df_time = df.set_index('date').copy()

# Daily case counts
daily_cases = df_time['cases'].resample('D').sum().fillna(0)

# Weekly aggregation
weekly_cases = df_time['cases'].resample('W').sum()
weekly_diseases = df_time.groupby(pd.Grouper(freq='W'))['disease'].count()

# Calculate moving averages for trend analysis
daily_cases_ma = daily_cases.rolling(window=3, center=True).mean()

# Create time series visualization
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Plot 1: Daily cases
axes[0, 0].plot(daily_cases.index, daily_cases.values, 'b-', alpha=0.7, label='Daily Cases')
axes[0, 0].plot(daily_cases_ma.index, daily_cases_ma.values, 'r-', linewidth=2, label='3-day Moving Average')
axes[0, 0].set_title('Daily Case Counts')
axes[0, 0].set_xlabel('Date')
axes[0, 0].set_ylabel('Cases')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# Plot 2: Weekly cases
axes[0, 1].bar(weekly_cases.index, weekly_cases.values, alpha=0.7, color='green')
axes[0, 1].set_title('Weekly Case Counts')
axes[0, 1].set_xlabel('Week')
axes[0, 1].set_ylabel('Cases')
axes[0, 1].grid(True, alpha=0.3)

# Plot 3: Disease frequency over time
axes[1, 0].plot(weekly_diseases.index, weekly_diseases.values, 'purple', linewidth=2)
axes[1, 0].set_title('Weekly Disease Reports')
axes[1, 0].set_xlabel('Week')
axes[1, 0].set_ylabel('Number of Reports')
axes[1, 0].grid(True, alpha=0.3)

# Plot 4: Severity distribution
severity_counts = df['severity'].value_counts()
axes[1, 1].pie(severity_counts.values, labels=severity_counts.index, autopct='%1.1f%%')
axes[1, 1].set_title('Outbreak Severity Distribution')

plt.tight_layout()
plt.savefig('time_series_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nğŸ“Š Time Series Summary:")
print(f"ğŸ“… Date range: {df['date'].min()} to {df['date'].max()}")
print(f"ğŸ“ˆ Total cases: {df['cases'].sum():,}")
print(f"ğŸ¥ Average daily cases: {daily_cases.mean():.1f}")
print(f"ğŸ“Š Peak daily cases: {daily_cases.max():,}")

# =============================================================================
# 5. MACHINE LEARNING: OUTBREAK CLASSIFICATION
# =============================================================================

print("\nğŸ¤– Preparing machine learning features...")

# Create features from headlines
def extract_features(headline):
    """
    Extract features from headline text for classification.
    
    Args:
        headline (str): News headline
    
    Returns:
        dict: Feature dictionary
    """
    
    # Text-based features
    features = {
        'length': len(headline),
        'word_count': len(headline.split()),
        'has_outbreak': int('outbreak' in headline.lower()),
        'has_cases': int('cases' in headline.lower()),
        'has_hits': int('hits' in headline.lower()),
        'has_rise': int('rise' in headline.lower()),
        'has_spike': int('spike' in headline.lower()),
        'has_reported': int('reported' in headline.lower()),
        'has_emergency': int('emergency' in headline.lower()),
        'has_warning': int('warning' in headline.lower()),
        'has_alert': int('alert' in headline.lower()),
        'exclamation_count': headline.count('!'),
        'uppercase_ratio': sum(1 for c in headline if c.isupper()) / len(headline)
    }
    
    return features

# Extract features for all headlines
feature_data = []
for headline in df['headline']:
    features = extract_features(headline)
    feature_data.append(features)

features_df = pd.DataFrame(feature_data)

# Create target variable (simulated - in real scenario, this would be labeled data)
# We'll use a simple heuristic: headlines with 'outbreak' or high case counts are 'real'
df['is_real_outbreak'] = ((df['cases'] > 100) | 
                          (df['headline'].str.contains('outbreak', case=False)) |
                          (df['severity'] == 'High')).astype(int)

# Combine features with target
X = features_df
y = df['is_real_outbreak']

print(f"ğŸ“Š Feature matrix shape: {X.shape}")
print(f"ğŸ¯ Target distribution: {y.value_counts().to_dict()}")

# Split data for training
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

print(f"\nğŸ“š Training set: {X_train.shape[0]} samples")
print(f"ğŸ§ª Test set: {X_test.shape[0]} samples")

# Train the classifier
print("\nğŸ¯ Training Random Forest Classifier...")

# Initialize and train the model
rf_classifier = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    random_state=42,
    n_jobs=-1
)

rf_classifier.fit(X_train, y_train)

# Make predictions
y_pred = rf_classifier.predict(X_test)
y_pred_proba = rf_classifier.predict_proba(X_test)[:, 1]

# Evaluate the model
print("\nğŸ“ˆ Model Performance:")
print("=" * 50)
print(classification_report(y_test, y_pred))

# Feature importance
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': rf_classifier.feature_importances_
}).sort_values('importance', ascending=False)

print("\nğŸ” Feature Importance:")
print(feature_importance.head(10))

# Visualize feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_importance['feature'][:10], feature_importance['importance'][:10])
plt.title('Top 10 Most Important Features')
plt.xlabel('Importance Score')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
plt.show()

# Confusion matrix
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['No Outbreak', 'Real Outbreak'],
            yticklabels=['No Outbreak', 'Real Outbreak'])
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
plt.show()

# =============================================================================
# 6. GEOSPATIAL VISUALIZATION
# =============================================================================

print("\nğŸ—ºï¸ Creating geospatial visualizations...")

# Filter for locations with valid coordinates
geo_df = df[df['latitude'].notna() & df['longitude'].notna()].copy()

if len(geo_df) > 0:
    # Create a simple map visualization
    fig, ax = plt.subplots(figsize=(12, 8))
    
    # Color code by severity
    colors = {'Low': 'green', 'Medium': 'orange', 'High': 'red'}
    
    for severity in geo_df['severity'].unique():
        subset = geo_df[geo_df['severity'] == severity]
        ax.scatter(subset['longitude'], subset['latitude'], 
                   c=colors[severity], s=subset['cases']/10, 
                   alpha=0.7, label=f'{severity} Severity')
    
    # Add labels for major outbreaks
    for idx, row in geo_df.iterrows():
        if row['cases'] > 500:  # Only label major outbreaks
            ax.annotate(f"{row['disease']}\n{row['cases']} cases", 
                        (row['longitude'], row['latitude']),
                        xytext=(5, 5), textcoords='offset points',
                        fontsize=8, bbox=dict(boxstyle='round,pad=0.3', 
                                              facecolor='white', alpha=0.8))
    
    ax.set_xlabel('Longitude')
    ax.set_ylabel('Latitude')
    ax.set_title('Global Outbreak Map')
    ax.legend()
    ax.grid(True, alpha=0.3)
    plt.savefig('global_outbreak_map.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"ğŸ“ Mapped {len(geo_df)} locations with valid coordinates")
else:
    print("âš ï¸ No locations with valid coordinates found for mapping")

# =============================================================================
# 7. DASHBOARD COMPONENTS
# =============================================================================

print("\nğŸ“Š Creating dashboard components...")

# Summary statistics
def create_summary_stats(df):
    """Create summary statistics for dashboard"""
    stats = {
        'Total Outbreaks': len(df),
        'Total Cases': df['cases'].sum(),
        'Average Cases per Outbreak': df['cases'].mean(),
        'High Severity Outbreaks': len(df[df['severity'] == 'High']),
        'Unique Diseases': df['disease'].nunique(),
        'Date Range': f"{df['date'].min().strftime('%Y-%m-%d')} to {df['date'].max().strftime('%Y-%m-%d')}"
    }
    return stats

# Disease breakdown
def create_disease_breakdown(df):
    """Create disease frequency analysis"""
    disease_stats = df.groupby('disease').agg({
        'cases': ['sum', 'mean', 'count'],
        'severity': lambda x: x.value_counts().index[0]
    }).round(2)
    
    disease_stats.columns = ['Total Cases', 'Avg Cases', 'Outbreak Count', 'Most Common Severity']
    return disease_stats.sort_values('Total Cases', ascending=False)

# Severity analysis
def create_severity_analysis(df):
    """Create severity level analysis"""
    severity_stats = df.groupby('severity').agg({
        'cases': ['sum', 'mean', 'count'],
        'disease': 'nunique'
    }).round(2)
    
    severity_stats.columns = ['Total Cases', 'Avg Cases', 'Outbreak Count', 'Unique Diseases']
    return severity_stats

# Generate dashboard data
summary_stats = create_summary_stats(df)
disease_breakdown = create_disease_breakdown(df)
severity_analysis = create_severity_analysis(df)

# Display dashboard components
print("\nğŸ“ˆ SUMMARY STATISTICS")
print("=" * 40)
for key, value in summary_stats.items():
    print(f"{key}: {value}")

print("\nğŸ¥ DISEASE BREAKDOWN")
print("=" * 40)
print(disease_breakdown)

print("\nâš ï¸ SEVERITY ANALYSIS")
print("=" * 40)
print(severity_analysis)

# Create visual dashboard
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# Plot 1: Disease distribution
disease_counts = df['disease'].value_counts()
axes[0, 0].barh(disease_counts.index, disease_counts.values)
axes[0, 0].set_title('Outbreaks by Disease')
axes[0, 0].set_xlabel('Number of Outbreaks')

# Plot 2: Severity distribution
severity_counts = df['severity'].value_counts()
axes[0, 1].pie(severity_counts.values, labels=severity_counts.index, autopct='%1.1f%%')
axes[0, 1].set_title('Outbreak Severity Distribution')

# Plot 3: Cases by disease
disease_cases = df.groupby('disease')['cases'].sum().sort_values(ascending=True)
axes[1, 0].barh(disease_cases.index, disease_cases.values)
axes[1, 0].set_title('Total Cases by Disease')
axes[1, 0].set_xlabel('Total Cases')

# Plot 4: Timeline of cases
axes[1, 1].scatter(df['date'], df['cases'], c=df['severity'].map({'Low': 'green', 'Medium': 'orange', 'High': 'red'}),
                    s=df['cases']/10, alpha=0.7)
axes[1, 1].set_title('Cases Over Time')
axes[1, 1].set_xlabel('Date')
axes[1, 1].set_ylabel('Cases')
axes[1, 1].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.savefig('dashboard_overview.png', dpi=300, bbox_inches='tight')
plt.show()

# =============================================================================
# 8. PREDICTIVE ANALYTICS
# =============================================================================

print("\nğŸ”® Implementing predictive analytics...")

# Simple trend analysis
def analyze_trends(df):
    """Analyze trends in the outbreak data"""
    
    # Daily case trends
    daily_data = df.set_index('date')['cases'].resample('D').sum().fillna(0)
    
    # Calculate trend indicators
    trend_indicators = {
        'total_cases': daily_data.sum(),
        'avg_daily_cases': daily_data.mean(),
        'peak_daily_cases': daily_data.max(),
        'trend_direction': 'increasing' if daily_data.iloc[-1] > daily_data.iloc[0] else 'decreasing',
        'volatility': daily_data.std(),
        'days_with_cases': (daily_data > 0).sum()
    }
    
    return trend_indicators, daily_data

# Risk assessment
def assess_risk(df):
    """Assess risk levels for different diseases and locations"""
    
    # Disease risk assessment
    disease_risk = df.groupby('disease').agg({
        'cases': ['sum', 'mean', 'max'],
        'severity': lambda x: (x == 'High').sum() / len(x)
    }).round(3)
    
    disease_risk.columns = ['Total Cases', 'Avg Cases', 'Max Cases', 'High Severity Rate']
    
    # Calculate risk score (simple heuristic)
    disease_risk['Risk Score'] = (
        disease_risk['Total Cases'] * 0.4 +
        disease_risk['High Severity Rate'] * 0.6
    ).round(3)
    
    return disease_risk.sort_values('Risk Score', ascending=False)

# Generate predictions
trend_indicators, daily_data = analyze_trends(df)
risk_assessment = assess_risk(df)

# Display results
print("\nğŸ“Š TREND ANALYSIS")
print("=" * 40)
for key, value in trend_indicators.items():
    print(f"{key.replace('_', ' ').title()}: {value}")

print("\nâš ï¸ RISK ASSESSMENT")
print("=" * 40)
print(risk_assessment)

# Visualize trends
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Plot 1: Daily case trends
axes[0].plot(daily_data.index, daily_data.values, 'b-', linewidth=2)
axes[0].set_title('Daily Case Trends')
axes[0].set_xlabel('Date')
axes[0].set_ylabel('Cases')
axes[0].grid(True, alpha=0.3)

# Plot 2: Risk scores
risk_scores = risk_assessment['Risk Score'].head(10)
axes[1].barh(risk_scores.index, risk_scores.values, color='red', alpha=0.7)
axes[1].set_title('Disease Risk Scores')
axes[1].set_xlabel('Risk Score')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('predictive_analytics.png', dpi=300, bbox_inches='tight')
plt.show()

# =============================================================================
# 9. EXPORT AND SUMMARY
# =============================================================================

print("\nğŸ’¾ Exporting results...")

# Create summary report
summary_report = {
    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
    'total_outbreaks': len(df),
    'total_cases': df['cases'].sum(),
    'unique_diseases': df['disease'].nunique(),
    'date_range': f"{df['date'].min().strftime('%Y-%m-%d')} to {df['date'].max().strftime('%Y-%m-%d')}",
    'model_accuracy': f"{rf_classifier.score(X_test, y_test):.3f}",
    'geocoded_locations': len(df[df['confidence'] == 'high']),
    'high_severity_outbreaks': len(df[df['severity'] == 'High']),
    'trend_direction': trend_indicators['trend_direction'],
    'top_risk_disease': risk_assessment.index[0] if len(risk_assessment) > 0 else 'N/A'
}

# Save processed data
df.to_csv('processed_outbreak_data.csv', index=False)
disease_breakdown.to_csv('disease_analysis.csv')
risk_assessment.to_csv('risk_assessment.csv')

print("âœ… Data exported successfully!")
print("\nğŸ“‹ SUMMARY REPORT")
print("=" * 50)
for key, value in summary_report.items():
    print(f"{key.replace('_', ' ').title()}: {value}")

print("\nğŸ¯ KEY INSIGHTS:")
print("=" * 50)
print(f"â€¢ Processed {len(df)} outbreak reports")
print(f"â€¢ Identified {df['disease'].nunique()} different diseases")
print(f"â€¢ {len(df[df['severity'] == 'High'])} high-severity outbreaks detected")
print(f"â€¢ Model achieved {rf_classifier.score(X_test, y_test):.1%} accuracy")
print(f"â€¢ Overall trend: {trend_indicators['trend_direction']}")
if len(risk_assessment) > 0:
    print(f"â€¢ Highest risk disease: {risk_assessment.index[0]}")

print("\nğŸš€ NEXT STEPS:")
print("=" * 50)
print("â€¢ Deploy model to production environment")
print("â€¢ Set up real-time data feeds")
print("â€¢ Implement automated alerts")
print("â€¢ Create interactive dashboard")
print("â€¢ Validate with real-world data")

print("\nğŸ‰ Pipeline execution complete!")
print("This enhanced outbreak monitoring system is ready for deployment.")

print("\nğŸ“ Generated Files:")
print("â€¢ processed_outbreak_data.csv - Main dataset")
print("â€¢ disease_analysis.csv - Disease breakdown")
print("â€¢ risk_assessment.csv - Risk analysis")
print("â€¢ *.png - Visualization plots") 